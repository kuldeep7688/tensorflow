{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST TUTORIAL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow relies on a highly efficient C++ backend to do its computation. The connection to this backend is called a session. The common usage for TensorFlow programs is to first create a graph and then launch it in a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do efficient numerical computing in Python, we typically use libraries like NumPy that do expensive operations such as matrix multiplication outside Python, using highly efficient code implemented in another language. Unfortunately, there can still be a lot of overhead from switching back to Python every operation. This overhead is especially bad if you want to run computations on GPUs or in a distributed manner, where there can be a high cost to transferring data.\n",
    "\n",
    "TensorFlow also does its heavy lifting outside Python, but it takes things a step further to avoid this overhead. Instead of running a single expensive operation independently from Python, TensorFlow lets us describe a graph of interacting operations that run entirely outside Python. This approach is similar to that used in Theano or Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "  batch = mnist.train.next_batch(100)\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9148\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "array([[[[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.01176471],\n",
      "         [ 0.60392159],\n",
      "         [ 0.99607849],\n",
      "         [ 0.70588237],\n",
      "         [ 0.07058824],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.27450982],\n",
      "         [ 0.71764708],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.76078439],\n",
      "         [ 0.70588237],\n",
      "         [ 0.14509805],\n",
      "         [ 0.02745098],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.10980393],\n",
      "         [ 0.41960788],\n",
      "         [ 0.97647065],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.65490198],\n",
      "         [ 0.02745098],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.34509805],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.59607846],\n",
      "         [ 0.04705883],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.27058825],\n",
      "         [ 0.9450981 ],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.67843139],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.34901962],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.29019609],\n",
      "         [ 0.92156869],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.91764712],\n",
      "         [ 0.73333335],\n",
      "         [ 0.15686275],\n",
      "         [ 0.05490196],\n",
      "         [ 0.58823532],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.34901962],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.27058825],\n",
      "         [ 0.92156869],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.60392159],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.23137257],\n",
      "         [ 0.92549026],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.34901962],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.11764707],\n",
      "         [ 0.9450981 ],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.90588242],\n",
      "         [ 0.29803923],\n",
      "         [ 0.05882353],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.47058827],\n",
      "         [ 0.98039222],\n",
      "         [ 0.99215692],\n",
      "         [ 0.91764712],\n",
      "         [ 0.25882354],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.52549022],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.27450982],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.10196079],\n",
      "         [ 0.80784321],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.7019608 ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.02352941],\n",
      "         [ 0.59215689],\n",
      "         [ 0.98431379],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.52941179],\n",
      "         [ 0.02745098],\n",
      "         [ 0.00392157],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.78823537],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.80392164],\n",
      "         [ 0.05882353],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.08627451],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.98431379],\n",
      "         [ 0.60392159],\n",
      "         [ 0.03137255],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.00392157],\n",
      "         [ 0.32941177],\n",
      "         [ 0.9333334 ],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.41568631],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.70588237],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.54509807],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.34901962],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.90196085],\n",
      "         [ 0.25490198],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.32941177],\n",
      "         [ 0.92156869],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.9450981 ],\n",
      "         [ 0.14117648],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.00392157],\n",
      "         [ 0.34901962],\n",
      "         [ 0.92941183],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.94117653],\n",
      "         [ 0.27450982],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.99607849],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.9333334 ],\n",
      "         [ 0.27843139],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.14901961],\n",
      "         [ 0.47058827],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.91764712],\n",
      "         [ 0.26274511],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 1.        ],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.86274517],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.42745101],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.91764712],\n",
      "         [ 0.27843139],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.99607849],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.86274517],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.13725491],\n",
      "         [ 0.51372552],\n",
      "         [ 0.98039222],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.60784316],\n",
      "         [ 0.26274511],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 1.        ],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.89019614],\n",
      "         [ 0.53725493],\n",
      "         [ 0.8705883 ],\n",
      "         [ 0.8705883 ],\n",
      "         [ 0.90196085],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.81568635],\n",
      "         [ 0.1254902 ],\n",
      "         [ 0.03137255],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.99607849],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.80392164],\n",
      "         [ 0.13333334],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.70588237],\n",
      "         [ 0.90980399],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.91764712],\n",
      "         [ 0.7019608 ],\n",
      "         [ 0.05882353],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.29411766],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.99215692],\n",
      "         [ 0.86666673],\n",
      "         [ 0.34901962],\n",
      "         [ 0.25882354],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]],\n",
      "\n",
      "        [[ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ],\n",
      "         [ 0.        ]]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import numpy as np\n",
    "pprint.pprint(mnist.train.images[0].shape)\n",
    "\n",
    "#pprint.pprint(np.reshape(mnist.train.images[0], [-1,28,28,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a multilayer convolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input. Our pooling is plain old max pooling over 2x2 blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], \n",
    "                             strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first convolutional layer\n",
    "\n",
    "The convolution will compute 32 features for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32]. The first two dimensions are the patch size, the next is the number of input channels, and the last is the number of output channels. We will also have a bias vector with a component for each output channel.\n",
    "\n",
    "\n",
    "To apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# reshaping x \n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second convolutional layer\n",
    "\n",
    "In order to build a deep network, we stack several layers of this type. The second layer will have 64 features for each 5x5 patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densely Connected Layer\n",
    "\n",
    "Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons to allow processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, and apply a ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7*7*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "To reduce overfitting, we will apply dropout before the readout layer. We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing. TensorFlow's tf.nn.dropout op automatically handles scaling neuron outputs in addition to masking them, so dropout just works without any additional scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " second densely connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readout Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 && train_accuracy = 0.08\n",
      "step = 100 && train_accuracy = 0.82\n",
      "step = 200 && train_accuracy = 0.9\n",
      "step = 300 && train_accuracy = 0.96\n",
      "step = 400 && train_accuracy = 0.92\n",
      "step = 500 && train_accuracy = 0.96\n",
      "step = 600 && train_accuracy = 0.96\n",
      "step = 700 && train_accuracy = 1\n",
      "step = 800 && train_accuracy = 0.92\n",
      "step = 900 && train_accuracy = 0.98\n",
      "step = 1000 && train_accuracy = 0.96\n",
      "step = 1100 && train_accuracy = 0.94\n",
      "step = 1200 && train_accuracy = 1\n",
      "step = 1300 && train_accuracy = 0.96\n",
      "step = 1400 && train_accuracy = 1\n",
      "step = 1500 && train_accuracy = 1\n",
      "step = 1600 && train_accuracy = 0.98\n",
      "step = 1700 && train_accuracy = 0.96\n",
      "step = 1800 && train_accuracy = 1\n",
      "step = 1900 && train_accuracy = 0.98\n",
      "step = 2000 && train_accuracy = 0.96\n",
      "step = 2100 && train_accuracy = 0.96\n",
      "step = 2200 && train_accuracy = 0.96\n",
      "step = 2300 && train_accuracy = 0.96\n",
      "step = 2400 && train_accuracy = 1\n",
      "step = 2500 && train_accuracy = 1\n",
      "step = 2600 && train_accuracy = 0.98\n",
      "step = 2700 && train_accuracy = 0.98\n",
      "step = 2800 && train_accuracy = 0.96\n",
      "step = 2900 && train_accuracy = 0.98\n",
      "step = 3000 && train_accuracy = 1\n",
      "step = 3100 && train_accuracy = 0.98\n",
      "step = 3200 && train_accuracy = 0.98\n",
      "step = 3300 && train_accuracy = 0.96\n",
      "step = 3400 && train_accuracy = 1\n",
      "step = 3500 && train_accuracy = 1\n",
      "step = 3600 && train_accuracy = 0.98\n",
      "step = 3700 && train_accuracy = 0.98\n",
      "step = 3800 && train_accuracy = 1\n",
      "step = 3900 && train_accuracy = 1\n",
      "step = 4000 && train_accuracy = 1\n",
      "step = 4100 && train_accuracy = 0.96\n",
      "step = 4200 && train_accuracy = 1\n",
      "step = 4300 && train_accuracy = 0.94\n",
      "step = 4400 && train_accuracy = 1\n",
      "step = 4500 && train_accuracy = 1\n",
      "step = 4600 && train_accuracy = 0.98\n",
      "step = 4700 && train_accuracy = 1\n",
      "step = 4800 && train_accuracy = 1\n",
      "step = 4900 && train_accuracy = 0.96\n",
      "step = 5000 && train_accuracy = 0.98\n",
      "step = 5100 && train_accuracy = 0.98\n",
      "step = 5200 && train_accuracy = 0.98\n",
      "step = 5300 && train_accuracy = 1\n",
      "step = 5400 && train_accuracy = 0.98\n",
      "step = 5500 && train_accuracy = 1\n",
      "step = 5600 && train_accuracy = 1\n",
      "step = 5700 && train_accuracy = 1\n",
      "step = 5800 && train_accuracy = 0.98\n",
      "step = 5900 && train_accuracy = 1\n",
      "step = 6000 && train_accuracy = 0.96\n",
      "step = 6100 && train_accuracy = 1\n",
      "step = 6200 && train_accuracy = 1\n",
      "step = 6300 && train_accuracy = 1\n",
      "step = 6400 && train_accuracy = 0.96\n",
      "step = 6500 && train_accuracy = 1\n",
      "step = 6600 && train_accuracy = 1\n",
      "step = 6700 && train_accuracy = 1\n",
      "step = 6800 && train_accuracy = 1\n",
      "step = 6900 && train_accuracy = 1\n",
      "step = 7000 && train_accuracy = 1\n",
      "step = 7100 && train_accuracy = 0.98\n",
      "step = 7200 && train_accuracy = 1\n",
      "step = 7300 && train_accuracy = 1\n",
      "step = 7400 && train_accuracy = 1\n",
      "step = 7500 && train_accuracy = 1\n",
      "step = 7600 && train_accuracy = 1\n",
      "step = 7700 && train_accuracy = 1\n",
      "step = 7800 && train_accuracy = 1\n",
      "step = 7900 && train_accuracy = 1\n",
      "step = 8000 && train_accuracy = 1\n",
      "step = 8100 && train_accuracy = 1\n",
      "step = 8200 && train_accuracy = 1\n",
      "step = 8300 && train_accuracy = 1\n",
      "step = 8400 && train_accuracy = 1\n",
      "step = 8500 && train_accuracy = 1\n",
      "step = 8600 && train_accuracy = 1\n",
      "step = 8700 && train_accuracy = 1\n",
      "step = 8800 && train_accuracy = 1\n",
      "step = 8900 && train_accuracy = 1\n",
      "step = 9000 && train_accuracy = 1\n",
      "step = 9100 && train_accuracy = 1\n",
      "step = 9200 && train_accuracy = 1\n",
      "step = 9300 && train_accuracy = 1\n",
      "step = 9400 && train_accuracy = 1\n",
      "step = 9500 && train_accuracy = 0.96\n",
      "step = 9600 && train_accuracy = 1\n",
      "step = 9700 && train_accuracy = 1\n",
      "step = 9800 && train_accuracy = 0.98\n",
      "step = 9900 && train_accuracy = 1\n",
      "step = 10000 && train_accuracy = 1\n",
      "step = 10100 && train_accuracy = 1\n",
      "step = 10200 && train_accuracy = 1\n",
      "step = 10300 && train_accuracy = 1\n",
      "step = 10400 && train_accuracy = 0.98\n",
      "step = 10500 && train_accuracy = 0.98\n",
      "step = 10600 && train_accuracy = 1\n",
      "step = 10700 && train_accuracy = 0.98\n",
      "step = 10800 && train_accuracy = 1\n",
      "step = 10900 && train_accuracy = 1\n",
      "step = 11000 && train_accuracy = 1\n",
      "step = 11100 && train_accuracy = 1\n",
      "step = 11200 && train_accuracy = 1\n",
      "step = 11300 && train_accuracy = 1\n",
      "step = 11400 && train_accuracy = 0.98\n",
      "step = 11500 && train_accuracy = 1\n",
      "step = 11600 && train_accuracy = 1\n",
      "step = 11700 && train_accuracy = 1\n",
      "step = 11800 && train_accuracy = 1\n",
      "step = 11900 && train_accuracy = 1\n",
      "step = 12000 && train_accuracy = 1\n",
      "step = 12100 && train_accuracy = 1\n",
      "step = 12200 && train_accuracy = 1\n",
      "step = 12300 && train_accuracy = 1\n",
      "step = 12400 && train_accuracy = 1\n",
      "step = 12500 && train_accuracy = 1\n",
      "step = 12600 && train_accuracy = 1\n",
      "step = 12700 && train_accuracy = 1\n",
      "step = 12800 && train_accuracy = 1\n",
      "step = 12900 && train_accuracy = 1\n",
      "step = 13000 && train_accuracy = 1\n",
      "step = 13100 && train_accuracy = 1\n",
      "step = 13200 && train_accuracy = 1\n",
      "step = 13300 && train_accuracy = 1\n",
      "step = 13400 && train_accuracy = 1\n",
      "step = 13500 && train_accuracy = 1\n",
      "step = 13600 && train_accuracy = 1\n",
      "step = 13700 && train_accuracy = 1\n",
      "step = 13800 && train_accuracy = 1\n",
      "step = 13900 && train_accuracy = 1\n",
      "step = 14000 && train_accuracy = 1\n",
      "step = 14100 && train_accuracy = 1\n",
      "step = 14200 && train_accuracy = 1\n",
      "step = 14300 && train_accuracy = 1\n",
      "step = 14400 && train_accuracy = 1\n",
      "step = 14500 && train_accuracy = 1\n",
      "step = 14600 && train_accuracy = 1\n",
      "step = 14700 && train_accuracy = 1\n",
      "step = 14800 && train_accuracy = 1\n",
      "step = 14900 && train_accuracy = 1\n",
      "step = 15000 && train_accuracy = 1\n",
      "step = 15100 && train_accuracy = 1\n",
      "step = 15200 && train_accuracy = 1\n",
      "step = 15300 && train_accuracy = 1\n",
      "step = 15400 && train_accuracy = 1\n",
      "step = 15500 && train_accuracy = 1\n",
      "step = 15600 && train_accuracy = 1\n",
      "step = 15700 && train_accuracy = 1\n",
      "step = 15800 && train_accuracy = 1\n",
      "step = 15900 && train_accuracy = 1\n",
      "step = 16000 && train_accuracy = 1\n",
      "step = 16100 && train_accuracy = 0.98\n",
      "step = 16200 && train_accuracy = 1\n",
      "step = 16300 && train_accuracy = 1\n",
      "step = 16400 && train_accuracy = 1\n",
      "step = 16500 && train_accuracy = 1\n",
      "step = 16600 && train_accuracy = 1\n",
      "step = 16700 && train_accuracy = 1\n",
      "step = 16800 && train_accuracy = 1\n",
      "step = 16900 && train_accuracy = 1\n",
      "step = 17000 && train_accuracy = 1\n",
      "step = 17100 && train_accuracy = 0.98\n",
      "step = 17200 && train_accuracy = 0.98\n",
      "step = 17300 && train_accuracy = 1\n",
      "step = 17400 && train_accuracy = 1\n",
      "step = 17500 && train_accuracy = 1\n",
      "step = 17600 && train_accuracy = 1\n",
      "step = 17700 && train_accuracy = 0.98\n",
      "step = 17800 && train_accuracy = 0.98\n",
      "step = 17900 && train_accuracy = 1\n",
      "step = 18000 && train_accuracy = 1\n",
      "step = 18100 && train_accuracy = 1\n",
      "step = 18200 && train_accuracy = 1\n",
      "step = 18300 && train_accuracy = 1\n",
      "step = 18400 && train_accuracy = 1\n",
      "step = 18500 && train_accuracy = 1\n",
      "step = 18600 && train_accuracy = 1\n",
      "step = 18700 && train_accuracy = 1\n",
      "step = 18800 && train_accuracy = 1\n",
      "step = 18900 && train_accuracy = 1\n",
      "step = 19000 && train_accuracy = 0.98\n",
      "step = 19100 && train_accuracy = 1\n",
      "step = 19200 && train_accuracy = 1\n",
      "step = 19300 && train_accuracy = 1\n",
      "step = 19400 && train_accuracy = 1\n",
      "step = 19500 && train_accuracy = 1\n",
      "step = 19600 && train_accuracy = 1\n",
      "step = 19700 && train_accuracy = 0.98\n",
      "step = 19800 && train_accuracy = 1\n",
      "step = 19900 && train_accuracy = 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(20000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(\n",
    "            feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            \n",
    "            print \"step = %d && train_accuracy = %g\" %(i, train_accuracy)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    print \"Test accuracy == %g\" %(accuracy.eval(\n",
    "        feed_dict={x:mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
