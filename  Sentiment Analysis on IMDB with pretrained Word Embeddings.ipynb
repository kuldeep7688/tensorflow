{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pyprind\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading glove embeddings as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print \"Loading Glove Model\"\n",
    "    pbar = pyprind.ProgBar(400000)\n",
    "    f = open(gloveFile,'rb')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "        pbar.update()\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400000  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:00:35\n"
     ]
    }
   ],
   "source": [
    "glove_file_name = \"/home/kuldeep.singh/clarity_social_media_analysis/imdb_movie_data_project/glove/glove.6B.300d.txt\"\n",
    "glove_model = loadGloveModel(glove_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading imdb movie review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Usually I love Lesbian movies even when they a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prix de Beauté was made on the cusp of the cha...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This lasted several years despite the late hou...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is 2009 and this way underrated gem has l...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Waters has given us a genuinely enjoyable...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Usually I love Lesbian movies even when they a...        0.0\n",
       "1  Prix de Beauté was made on the cusp of the cha...        1.0\n",
       "2  This lasted several years despite the late hou...        1.0\n",
       "3  This is 2009 and this way underrated gem has l...        1.0\n",
       "4  John Waters has given us a genuinely enjoyable...        1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/kuldeep.singh/clarity_social_media_analysis/imdb_movie_data_project/movie_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for preprocessing the movie reviews\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    text = re.sub(strip_special_chars, \" \", text.lower())\n",
    "    text = re.sub(\"  \", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.review = df.review.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'usually i love lesbian movies even when they are not very good i m biased i guess but this one is just the pits yes the scenery and the buildings are beautiful and there is a brief but beautiful erotic interlude but otherwise this movie is just a complete waste of time annamarie alternates between sulking and getting high stoned passing out on whatever drug or booze is handy and ella inexplicably puts up with this abominable behavior through the entire movie at no time are we given any insight into why this is so or even why annamarie is so depressed and withdrawn if there had at least been some kind of closure in the potentially romantic we don t even know relationship between the two there might have been some kind of satisfaction but although annamarie at one point asks ella why do you love me ella doesn t even acknowledge this it s never really clear whether this is anything more than an ill behaved lesbian on a boring road trip with a straight woman even the interactions between the two women and the local people they meet on the journey which could have been lively and informative are instead flat tedious and mostly incomprehensible there is one good joke in the movie although i m sure it was unintentional the women travel in a two seat ford coupe with a middling sized trunk yet when they set up camp they have an enormous tent cots sleeping gear and even a table chair and typewriter on top of that when they board a ferry we see piles of luggage presumably theirs presumably also carried in the little ford s trunk and through the entire film we never see one gas station or anywhere that looks like it would actually have any place to buy gasoline mostly they travel through endless miles of desolate desert so where did they get fuel there may not be too many lesbian films out there good or bad but there are plenty that are better than this and very few that are worse leave this one in the rack'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining max number of features to be extracted from count vectorizer\n",
    "MAX_FEATURES = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(max_features=MAX_FEATURES)\n",
    "docs = np.array(df.review)\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "word2index_map = count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, unicode)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(glove_model.keys()[1]), type(word2index_map.keys()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sunda', array([  2.53770000e-01,  -2.77290000e-01,  -3.70360000e-01,\n",
       "         -2.48790000e-01,   1.03050000e+00,  -8.03160000e-01,\n",
       "          3.72680000e-01,   1.62870000e-01,  -2.27360000e-01,\n",
       "          9.25530000e-02,  -1.06840000e-01,   8.18610000e-01,\n",
       "         -2.76340000e-01,  -3.77100000e-01,   2.92330000e-01,\n",
       "          1.35780000e-01,  -1.21440000e-01,   1.13000000e-01,\n",
       "          1.42080000e-01,  -3.21240000e-02,   7.37820000e-01,\n",
       "         -2.47010000e-01,   1.56340000e-01,   1.47730000e-01,\n",
       "         -1.06590000e-01,  -8.56000000e-02,  -5.97600000e-01,\n",
       "         -4.45970000e-01,   3.69810000e-02,   1.04570000e-01,\n",
       "          2.47820000e-01,   1.03020000e+00,   4.06550000e-02,\n",
       "          7.48950000e-02,   8.01150000e-01,  -1.64490000e-01,\n",
       "          5.78510000e-02,   2.19920000e-01,   2.37920000e-01,\n",
       "          3.68330000e-01,  -6.48590000e-02,   4.05760000e-02,\n",
       "         -3.03270000e-01,  -7.08010000e-01,   6.42880000e-01,\n",
       "         -2.87490000e-01,  -1.98730000e-02,   2.13390000e-02,\n",
       "         -5.56330000e-02,  -6.98520000e-01,   2.44190000e-01,\n",
       "         -6.90420000e-02,   1.24500000e-01,  -4.60580000e-01,\n",
       "         -5.26650000e-01,  -5.34170000e-01,  -1.92350000e-01,\n",
       "         -8.38320000e-01,   1.49000000e-01,   5.47540000e-01,\n",
       "         -1.52010000e-01,  -8.73510000e-01,   3.90110000e-01,\n",
       "          1.95170000e-01,   9.12800000e-01,   6.49910000e-01,\n",
       "          5.06260000e-01,   8.51150000e-01,   2.89210000e-01,\n",
       "          3.12950000e-01,   3.25430000e-02,   1.77090000e-01,\n",
       "          5.97700000e-01,  -5.18170000e-01,  -2.30780000e-01,\n",
       "          3.08980000e-01,   7.21610000e-02,  -4.67170000e-01,\n",
       "          1.38640000e-01,   4.67620000e-01,  -1.07310000e-01,\n",
       "         -1.64880000e-01,   1.54750000e-01,  -4.83210000e-02,\n",
       "         -2.10150000e-01,  -7.04220000e-01,   4.20910000e-02,\n",
       "          5.76050000e-01,   2.52500000e-01,  -3.91180000e-01,\n",
       "         -1.31060000e-01,  -2.68520000e-01,  -6.16920000e-01,\n",
       "          2.23330000e-01,   4.33260000e-01,   3.57090000e-01,\n",
       "          7.23020000e-01,  -1.87940000e-01,  -3.09140000e-01,\n",
       "          2.43930000e-01,   1.89450000e-01,  -3.24310000e-01,\n",
       "          5.00590000e-01,  -3.33230000e-01,  -2.73080000e-01,\n",
       "         -3.79780000e-01,   5.27140000e-01,   2.47080000e-01,\n",
       "          7.23820000e-01,   1.04370000e+00,   2.36970000e-01,\n",
       "          1.47690000e-02,  -7.74720000e-01,   2.53310000e-01,\n",
       "          7.53160000e-02,  -6.74610000e-01,  -4.18820000e-01,\n",
       "         -5.52610000e-01,  -1.05650000e-01,  -7.26650000e-01,\n",
       "         -2.17980000e-01,   1.59470000e-02,  -5.11250000e-01,\n",
       "         -5.81590000e-01,   2.37330000e-01,   3.35140000e-01,\n",
       "          3.10670000e-01,  -3.23270000e-01,   4.51950000e-01,\n",
       "         -4.20630000e-01,   2.63530000e-02,  -3.33150000e-02,\n",
       "          1.24720000e-01,   1.00580000e+00,   4.78000000e-01,\n",
       "         -3.37160000e-01,  -1.29900000e-02,   1.69900000e-01,\n",
       "          1.78490000e-01,   6.62280000e-01,   5.45040000e-01,\n",
       "          8.26710000e-02,  -1.23650000e-01,  -4.40660000e-01,\n",
       "          3.77700000e-01,  -1.93470000e-01,   5.38450000e-01,\n",
       "          2.23890000e-01,  -5.89580000e-01,  -6.79300000e-01,\n",
       "         -5.55550000e-01,   9.30340000e-02,   1.38290000e-01,\n",
       "          3.75240000e-01,  -5.21210000e-01,  -5.49320000e-01,\n",
       "          3.02440000e-01,  -3.85170000e-01,   2.00820000e-01,\n",
       "         -7.80030000e-01,  -3.58930000e-01,   2.57560000e-01,\n",
       "         -1.54770000e-03,   5.61570000e-02,  -3.54100000e-01,\n",
       "         -6.16700000e-01,  -4.62070000e-01,   5.44940000e-01,\n",
       "         -1.12830000e-03,  -4.91910000e-01,   5.31600000e-01,\n",
       "         -4.63920000e-02,   1.12570000e-01,  -4.84550000e-01,\n",
       "         -4.31650000e-01,   5.33580000e-02,   3.82190000e-01,\n",
       "          2.32670000e-01,   8.91760000e-02,  -2.69570000e-01,\n",
       "         -1.02580000e-01,   2.04610000e-02,  -1.19020000e-01,\n",
       "          2.92840000e-01,   7.02590000e-01,  -9.05900000e-01,\n",
       "          7.87970000e-01,   8.82280000e-01,  -7.01650000e-01,\n",
       "          1.51160000e-01,  -2.18710000e-03,   3.00220000e-01,\n",
       "          1.28300000e-01,  -7.60180000e-02,  -2.30500000e-01,\n",
       "         -8.05480000e-02,   2.30470000e-02,  -1.86240000e-01,\n",
       "          7.44200000e-02,   2.50740000e-01,   9.95850000e-02,\n",
       "         -1.97490000e-01,  -3.46660000e-02,   9.68590000e-01,\n",
       "          6.58260000e-01,   8.33590000e-01,   9.89420000e-01,\n",
       "         -9.02880000e-01,   9.22170000e-01,   3.80190000e-02,\n",
       "         -5.46150000e-01,   9.21860000e-01,   2.41440000e-01,\n",
       "          9.28460000e-03,  -6.75380000e-01,  -2.18770000e-01,\n",
       "         -4.19790000e-01,   3.88690000e-01,  -4.88310000e-01,\n",
       "          2.71600000e-01,  -1.92420000e-01,   3.70940000e-01,\n",
       "          5.45680000e-02,   2.16490000e-01,  -1.51640000e-01,\n",
       "         -6.79810000e-01,   2.86900000e-01,  -7.57540000e-02,\n",
       "          3.97060000e-01,  -4.42210000e-01,  -4.51750000e-01,\n",
       "          3.83990000e-02,  -5.15830000e-01,   1.43310000e-01,\n",
       "         -1.21070000e-01,   3.53100000e-01,   4.18940000e-02,\n",
       "          1.66450000e-01,   4.54890000e-01,   1.39810000e-02,\n",
       "         -6.94990000e-01,  -2.75870000e-01,  -4.82010000e-01,\n",
       "         -4.63460000e-01,  -6.42430000e-01,   4.35150000e-01,\n",
       "          2.00630000e-03,   3.16460000e-01,   1.78060000e-01,\n",
       "         -1.70900000e-01,  -4.09810000e-01,   1.11100000e-01,\n",
       "          4.53040000e-02,  -1.80100000e-01,  -7.82450000e-01,\n",
       "         -2.34250000e-01,  -1.42740000e-03,   4.69910000e-01,\n",
       "          3.02470000e-02,   5.01130000e-01,   3.68720000e-01,\n",
       "          1.08770000e-01,   2.70800000e-01,   1.81930000e-01,\n",
       "          2.86030000e-01,   1.12190000e-01,  -1.09750000e-01,\n",
       "         -1.10260000e+00,   2.52110000e-01,  -5.25650000e-01,\n",
       "          2.04100000e-01,  -1.12680000e+00,   9.03940000e-01,\n",
       "          4.15060000e-01,  -7.44570000e-01,   2.82870000e-01,\n",
       "          1.04880000e+00,  -5.89930000e-01,  -5.54310000e-01,\n",
       "         -2.66360000e-01,  -1.88010000e-01,   5.37680000e-01,\n",
       "          8.90920000e-02,  -6.34640000e-01,  -7.82700000e-01,\n",
       "         -1.02510000e+00,   5.31010000e-02,  -2.91560000e-01,\n",
       "          3.71120000e-01,  -1.36980000e+00,   1.31000000e-01,\n",
       "          3.77100000e-01,  -3.37730000e-02,   2.13970000e-01,\n",
       "         -1.38210000e+00,  -5.69960000e-02,   4.25620000e-01,\n",
       "          3.00380000e-01,  -5.76660000e-02,  -1.23820000e+00]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectors = glove_model.values()\n",
    "glove_model.keys()[399999], wordVectors[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2index_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ac2547263ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# updating the word2index_map for which glove_model has vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mupdated_word2index_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2index_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mupdated_word2index_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word2index_map' is not defined"
     ]
    }
   ],
   "source": [
    "# keeping only those words which are present in Glove Model dictionary\n",
    "# updating the word2index_map for which glove_model has vectors\n",
    "updated_word2index_map = {}\n",
    "for key, index in word2index_map.iteritems():\n",
    "    if str(key) in glove_model.keys():\n",
    "        updated_word2index_map[key] = index\n",
    "len(updated_word2index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining final vocabulary list to be used\n",
    "UPDATED_VOCABULORY = updated_word2index_map.keys()\n",
    "UPDATED_VOCABULORY.append(\"PAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparing final glove model for upadted vocabulary\n",
    "UPDATED_GLOVE_MODEL = {}\n",
    "for word in UPDATED_VOCABULORY:\n",
    "    if word is \"PAD\":\n",
    "        UPDATED_GLOVE_MODEL[word] = np.array([0]*glove_model[\"basketball\"].shape[0], dtype=np.float64)\n",
    "    else:\n",
    "        UPDATED_GLOVE_MODEL[word] = glove_model[word]\n",
    "\n",
    "len(UPDATED_GLOVE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding embedding for \"PAD\" token\n",
    "# UPDATED_GLOVE_MODEL[\"PAD\"] = np.array([10000]*300, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "wordVectors = UPDATED_GLOVE_MODEL.values()\n",
    "print type(wordVectors)\n",
    "print type(wordVectors[0])\n",
    "print type(wordVectors[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating final word2index_map and index2word_map using final glove model\n",
    "EMBEDDING_MATRIX = []\n",
    "UPDATED_WORD2INDEX_MAP = {}\n",
    "UPDATED_INDEX2WORD_MAP = {}\n",
    "\n",
    "for i, word in enumerate(UPDATED_VOCABULORY):\n",
    "    UPDATED_WORD2INDEX_MAP[word] = i\n",
    "    UPDATED_INDEX2WORD_MAP[i] = word\n",
    "    EMBEDDING_MATRIX.append(UPDATED_GLOVE_MODEL[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MATRIX = np.array(EMBEDDING_MATRIX, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using 20000 reviews for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:26\n"
     ]
    }
   ],
   "source": [
    "# preparing the input list of indices according to word embeddings \n",
    "data = []\n",
    "pbar = pyprind.ProgBar(len(df.review[:20000]))\n",
    "for review in df.review[:20000]:\n",
    "    temp_data = []\n",
    "    for word in review.lower().split():\n",
    "        if word in UPDATED_VOCABULORY:\n",
    "            temp_data.append(UPDATED_WORD2INDEX_MAP[word])\n",
    "    data.append(temp_data)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKEAAAJeCAYAAABs0xbRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSpP+dfwFSMU3EQtXvcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7556c80610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the length of sequences\n",
    "lenList = []\n",
    "for d in data:\n",
    "    counter = len(d)\n",
    "    lenList.append(counter)\n",
    "print len(lenList)\n",
    "\n",
    "#print lenList[:10]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(lenList, 100)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.sentiment[:20000]\n",
    "labels_one_hot = []\n",
    "for i in range(len(labels)):\n",
    "    label = int(labels[i])\n",
    "    one_hot = [0.0]*2\n",
    "    one_hot[label] = 1.0\n",
    "    labels_one_hot.append(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate batch \n",
    "def generate_batch(batch_size, data_x, data_y):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [data_x[i] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function used for tensorboard\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope(\"stddev\"):\n",
    "            stddev= tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar(\"stddev\", stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "embedding_dimension = 300\n",
    "hidden_layer_size = 128\n",
    "time_steps = 300\n",
    "vocabulary_size = len(UPDATED_VOCABULORY)\n",
    "\n",
    "LOG_DIR = \"/home/kuldeep.singh/books/deep learning /tensorflow_tutorials/logs/imdb_tensorflow_glove\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "for i in range(len(data)):\n",
    "    if len(data[i]) < 300:\n",
    "        data[i].extend([len(UPDATED_WORD2INDEX_MAP) - 1]*(300 - len(data[i])))\n",
    "    else:\n",
    "        data[i] = data[i][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = data[:10000]\n",
    "train_y = labels_one_hot[:10000]\n",
    "\n",
    "test_x = data[10000:15000]\n",
    "test_y = labels_one_hot[10000:15000]\n",
    "\n",
    "validation_x = data[15000:20000]\n",
    "validation_y = labels_one_hot[15000:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = tf.placeholder(tf.int32, shape=[None, time_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size, embedding_dimension])\n",
    "_keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if PRE_TRAINED:\n",
    "embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, embedding_dimension]), trainable=False)\n",
    "\n",
    "embedding_init = embeddings.assign(embedding_placeholder)\n",
    "embed = tf.nn.embedding_lookup(EMBEDDING_MATRIX, _inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(hidden_layer_size):\n",
    "    single_lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)\n",
    "    single_lstm_cell = tf.contrib.rnn.DropoutWrapper(single_lstm_cell,  output_keep_prob = _keep_prob)\n",
    "    return single_lstm_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"lstm\"):\n",
    "#     lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell(hidden_layer_size) for _ in range(num_lstm_layers)],\n",
    "                                       state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "#     outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed, dtype=tf.float32)\n",
    "\n",
    "weights = {\n",
    "    \"linear_layer\": tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], mean=0, stddev=0.01))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"linear_layer\": tf.Variable(tf.truncated_normal([num_classes], mean=0, stddev=0.01))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = tf.matmul(states[num_lstm_layers-1][1], weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=final_output, labels=_labels))\n",
    "    tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=final_output, labels=_labels))\n",
    "\n",
    "with tf.name_scope(\"train_step\"):\n",
    "    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(tf.argmax(final_output, 1), tf.argmax(_labels, 1))\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_predictions, tf.float32)))*100\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Minibatch loss: 0.692777, Training Accuracy: 54.68750\n",
      "Iteration: 100, Minibatch loss: 0.693716, Training Accuracy: 43.75000\n",
      "Iteration: 200, Minibatch loss: 0.680003, Training Accuracy: 64.06250\n",
      "Iteration: 300, Minibatch loss: 0.647529, Training Accuracy: 64.06250\n",
      "Iteration: 400, Minibatch loss: 0.682140, Training Accuracy: 62.50000\n",
      "Iteration: 500, Minibatch loss: 0.682808, Training Accuracy: 54.68750\n",
      "Iteration: 600, Minibatch loss: 0.671875, Training Accuracy: 60.93750\n",
      "Iteration: 700, Minibatch loss: 0.686757, Training Accuracy: 56.25000\n",
      "Iteration: 800, Minibatch loss: 0.667344, Training Accuracy: 60.93750\n",
      "Iteration: 900, Minibatch loss: 0.496791, Training Accuracy: 76.56250\n",
      "Iteration: 1000, Minibatch loss: 0.609121, Training Accuracy: 67.18750\n",
      "Iteration: 1100, Minibatch loss: 0.658843, Training Accuracy: 60.93750\n",
      "Iteration: 1200, Minibatch loss: 0.530011, Training Accuracy: 75.00000\n",
      "Iteration: 1300, Minibatch loss: 0.671328, Training Accuracy: 60.93750\n",
      "Iteration: 1400, Minibatch loss: 0.570281, Training Accuracy: 68.75000\n",
      "Iteration: 1500, Minibatch loss: 0.574154, Training Accuracy: 73.43750\n",
      "Iteration: 1600, Minibatch loss: 0.653171, Training Accuracy: 56.25000\n",
      "Iteration: 1700, Minibatch loss: 0.593957, Training Accuracy: 68.75000\n",
      "Iteration: 1800, Minibatch loss: 0.446256, Training Accuracy: 85.93750\n",
      "Iteration: 1900, Minibatch loss: 0.490146, Training Accuracy: 79.68750\n",
      "Iteration: 2000, Minibatch loss: 0.497675, Training Accuracy: 78.12500\n",
      "Iteration: 2100, Minibatch loss: 0.474158, Training Accuracy: 79.68750\n",
      "Iteration: 2200, Minibatch loss: 0.429069, Training Accuracy: 81.25000\n",
      "Iteration: 2300, Minibatch loss: 0.388368, Training Accuracy: 84.37500\n",
      "Iteration: 2400, Minibatch loss: 0.487124, Training Accuracy: 73.43750\n",
      "Iteration: 2500, Minibatch loss: 0.378045, Training Accuracy: 89.06250\n",
      "Iteration: 2600, Minibatch loss: 0.467068, Training Accuracy: 76.56250\n",
      "Iteration: 2700, Minibatch loss: 0.452012, Training Accuracy: 79.68750\n",
      "Iteration: 2800, Minibatch loss: 0.431157, Training Accuracy: 82.81250\n",
      "Iteration: 2900, Minibatch loss: 0.318544, Training Accuracy: 89.06250\n",
      "Iteration: 3000, Minibatch loss: 0.318420, Training Accuracy: 89.06250\n",
      "Iteration: 3100, Minibatch loss: 0.252407, Training Accuracy: 93.75000\n",
      "Iteration: 3200, Minibatch loss: 0.345573, Training Accuracy: 87.50000\n",
      "Iteration: 3300, Minibatch loss: 0.434283, Training Accuracy: 79.68750\n",
      "Iteration: 3400, Minibatch loss: 0.346385, Training Accuracy: 85.93750\n",
      "Iteration: 3500, Minibatch loss: 0.207402, Training Accuracy: 92.18750\n",
      "Iteration: 3600, Minibatch loss: 0.385526, Training Accuracy: 85.93750\n",
      "Iteration: 3700, Minibatch loss: 0.282141, Training Accuracy: 87.50000\n",
      "Iteration: 3800, Minibatch loss: 0.268158, Training Accuracy: 92.18750\n",
      "Iteration: 3900, Minibatch loss: 0.205216, Training Accuracy: 93.75000\n",
      "Iteration: 4000, Minibatch loss: 0.216000, Training Accuracy: 93.75000\n",
      "Iteration: 4100, Minibatch loss: 0.181974, Training Accuracy: 89.06250\n",
      "Iteration: 4200, Minibatch loss: 0.139275, Training Accuracy: 96.87500\n",
      "Iteration: 4300, Minibatch loss: 0.226270, Training Accuracy: 93.75000\n",
      "Iteration: 4400, Minibatch loss: 0.185677, Training Accuracy: 90.62500\n",
      "Iteration: 4500, Minibatch loss: 0.099573, Training Accuracy: 96.87500\n",
      "Iteration: 4600, Minibatch loss: 0.264751, Training Accuracy: 90.62500\n",
      "Iteration: 4700, Minibatch loss: 0.099477, Training Accuracy: 98.43750\n",
      "Iteration: 4800, Minibatch loss: 0.096667, Training Accuracy: 98.43750\n",
      "Iteration: 4900, Minibatch loss: 0.251243, Training Accuracy: 90.62500\n",
      "Test batch accuracy 0: 90.62500\n",
      "Test batch accuracy 1: 84.37500\n",
      "Test batch accuracy 2: 81.25000\n",
      "Test batch accuracy 3: 70.31250\n",
      "Test batch accuracy 4: 82.81250\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + \"/train\", graph=tf.get_default_graph())\n",
    "    validation_writer = tf.summary.FileWriter(LOG_DIR + \"/test\", graph=tf.get_default_graph())\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(5000):\n",
    "        x_batch, y_batch = generate_batch(batch_size, train_x, train_y)\n",
    "        \n",
    "        summary, _ = sess.run([merged, train_step], feed_dict={_inputs: x_batch, _labels: y_batch,\n",
    "                                                              _keep_prob: 0.75})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            acc, train_loss =  sess.run([accuracy, cross_entropy], feed_dict=\n",
    "                                        {_inputs: x_batch, _labels: y_batch,\n",
    "                                        _keep_prob: 0.75})\n",
    "            \n",
    "            print \"Iteration: \" + str(i) +\", Minibatch loss: {:.6f}\".format(train_loss)\\\n",
    "            + \", Training Accuracy: {:.5f}\".format(acc)\n",
    "        \n",
    "        if i%10:\n",
    "            x_val, y_val = generate_batch(batch_size, validation_x, validation_y)\n",
    "            summary, validation_acc = sess.run([merged, accuracy], \n",
    "                                            feed_dict={\n",
    "                                                _inputs:x_val, _labels:y_val,\n",
    "                                                _keep_prob: 1.0\n",
    "                                            })\n",
    "            validation_writer.add_summary(summary, i)\n",
    "        \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test = generate_batch(batch_size, test_x,test_y)\n",
    "        batch_acc, _ = sess.run([accuracy, train_step], feed_dict={\n",
    "            _inputs:x_test, _labels:y_test, _keep_prob: 1.0\n",
    "        })\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
